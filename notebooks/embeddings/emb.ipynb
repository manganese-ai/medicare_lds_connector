{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking into \n",
    "https://github.com/clinicalml/embeddings\n",
    "\n",
    "**To learn the embeddings**\n",
    "- The data of each individual is in a structured format which contains information including diagnosis codes (ICD-9), medical visits, lab test results (LOINC), and drug usage (NDC)\n",
    "- patient 123\n",
    "    - data are input in a timeline from (say) 2014-2016\n",
    "    - Data point 1 = screening procedure for the flu (CPT 87804)\n",
    "    - Data point 2 = prescription of acetaminophen (NDC 57344-0001)\n",
    "    - Data point 3 = lab result of elevated white count (LOINC 26464-8)\n",
    "    - Data point 4 = diagnosis of acute leukemia (ICD-9 208.0)\n",
    "    - Data point 5 = chemotherapy administration (CPT 96416)\n",
    "\n",
    "**Repo Structure**\n",
    "Text file(s) of embeddings\n",
    "- `claims_codes_hs_300.txt`: **dimensions 51327 x 300**\n",
    "    - ICD-9 diagnosis (CM) and procedure (PCS) codes\n",
    "    - NDC medication codes\n",
    "    - LOINC laboratory codes\n",
    "    - sample = 8 years of claims for 4 million people\n",
    "    - **this is the one we are interested in**\n",
    "- `claims_cuis_hs_300.txt`: dimensions 14852 x 300\n",
    "    - UMLS concept unique identifiers\n",
    "    - I think this is the same sample as above (8 years of claims for 4 mil)\n",
    "- `DeVine_etal_200.txt`: dimsensions 52102 x 200\n",
    "    - UMLS concept unique identifiers\n",
    "    - sample: ~350,000 medical journal abstracts\n",
    "- `stanford_cuis_svd_300.txt`: dimensions 22705 x 300\n",
    "    - UMLS concept unique identifiers\n",
    "    - sample: 20 million clinical notes from 19 years of data\n",
    "\n",
    "`eval`\n",
    "- ignoring files for CUI (`2a_concept_ID_to_string.txt.gz`, `2b_concept_ID_to_CUI.txt`, `ccs_coarsest.txt`, `cui_icd9.txt`, `may_prevent_cui.txt`, `convert_code_to_cui_embedding.py`, `ndfrt_analysis.py`, `visualize_stanford_embeddings.py`)\n",
    "- data\n",
    "    - `CMS32_DESC_LONG_DX.txt`: ICD-9 cpdes (?), e.g., 0010 = cholera due to vibrio cholerae; 0011 = cholera due to vibrio cholerae el tor\n",
    "    - `cpt_code_names.txt`: CPT codes\n",
    "    - `icd9_grp_file.txt`: \n",
    "    - `icd9Tree.txt`: how ICDs are grouped, e.g., codes 001-009 are intestinal infectious diseases, where 001 = cholera and 001.0 = cholera d/t vib cholerae\n",
    "    - `ingredient_all_ndcs.txt`: NDC codes?\n",
    "    - `ingredient_drug_ndc.txt`: NDC codes?\n",
    "    - `ingredient_ndcs.txt`: NDC codes?\n",
    "    - `ndc_labels.txt`: NDC (national drug codes) with description (e.g., 0002060440---SEROMYCIN 250 MG PULVULE)\n",
    "    - `loinc_code_names.txt`: LOINC codes (laboratory procedures)\n",
    "    - `files_to_process.txt`: the embeddings you want to process (e.g., `../claims_cuis_hs_300.txt`)\n",
    "    - `codes.json`: \n",
    "- `codes_analysis.py`\n",
    "- `icd9.py`\n",
    "- `Embedding_Evaluation.ipynb`\n",
    "- `visualize_claims_embeddings.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from embed_func import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt, med, diag, proc, rx, inp, lab = discard_info(pt, med, diag, proc, rx, inp, lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a single patient\n",
    "p = pt.Patid.unique()[10]\n",
    "df = single_pt_wl(p, med, diag, proc, rx, inp, lab)\n",
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sent = single_pt_preprocess(df)\n",
    "[len(i) for i in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pats = pt.Patid.unique()\n",
    "docs = all_pt(pats, med, diag, proc, rx, inp, lab)\n",
    "\n",
    "len(docs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emb_input.json','w') as f:\n",
    "    json.dump(docs, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emb_input.json') as f:\n",
    "    emb_input = json.load(f)\n",
    "\n",
    "len(emb_input.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokensize -- don't need to here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok_ind dict\n",
    "\n",
    "# ind_tok_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate skip-gram\n",
    "# model = Word2Vec(\n",
    "#     sentences = sent,\n",
    "#     size = , # size of hidden layer\n",
    "#     window = , # size of skip-gram window\n",
    "#     min_count = , # sets min number of times a word has to appear\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code = # diagnosis code\n",
    "# model.wv['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.most_similar(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdoel.save(\"baseline_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
